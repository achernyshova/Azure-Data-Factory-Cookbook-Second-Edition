{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c108e71-5e7f-4e9e-9e3a-db1ab0e4ff8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 1: Set up the connection\n",
    "\n",
    "Assign access key and set spark configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c760fc54-dbf0-4715-8df6-8fdff699c029",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"adfcookbookv2sa\"\n",
    "storage_account_access_key = \"xxxx\"\n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.\"+storage_account_name+\".blob.core.windows.net\",\n",
    "  storage_account_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51114086-25f5-4c9c-8bb3-64ff28b0f0f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 2: Read the data\n",
    "\n",
    "Now that we have specified our file metadata, we can create a DataFrame. Notice that we use an *option* to specify that we want to infer the schema from the file. We can also explicitly set this to a particular schema if we have one already.\n",
    "\n",
    "First, let's create a DataFrame in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_container = 'raw'\n",
    "ratings_location = f'wasbs://{raw_container}@{storage_account_name}.blob.core.windows.net/ratings.csv'\n",
    "movies_location = f'wasbs://{raw_container}@{storage_account_name}.blob.core.windows.net/movies.csv'\n",
    "links_location = f'wasbs://{raw_container}@{storage_account_name}.blob.core.windows.net/links.csv'\n",
    "tags_location = f'wasbs://{raw_container}@{storage_account_name}.blob.core.windows.net/tags.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "623310dc-97db-4fd0-9103-2dd507b4b3d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ratingsDF = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(ratings_location)\n",
    "linksDF = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(links_location)\n",
    "moviesDF = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(movies_location)\n",
    "tagsDF = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(tags_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09108c89-6164-4e8c-823e-93b1abdeeed3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 3: Transform the data\n",
    "\n",
    "Use spark sql functions to clean, aggregate and transform the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097a4fa0-6685-4bf4-96b4-3e05051d1957",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, round, current_timestamp, concat_ws, collect_set, from_unixtime, max, date_format, to_date\n",
    "agg_ratingsDF = ratingsDF.groupBy('movieId').agg(round(avg('rating'), 2).alias(\"avg_rating\"), max('timestamp').alias(\"unix_timestamp\"))\n",
    "agg_tagsDF = tagsDF.groupBy('movieId').agg(concat_ws(' | ', collect_set('tag')).alias('agg_tags'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c7cb62-c6c0-4f22-90c8-d37f864fa7dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "resultDF = (moviesDF.join(agg_ratingsDF, ['movieId'], 'left')\n",
    "                  .join(linksDF, ['movieId'], 'left')\n",
    "                  .join(agg_tagsDF, ['movieId'], 'left')\n",
    "                  .withColumn(\"rating_datetime\", to_date(date_format(from_unixtime(\"unix_timestamp\"), \"yyyy-MM-dd\")))\n",
    "                  .withColumn('load_timestamp', current_timestamp())\n",
    "                  .drop('unix_timestamp')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993d3d26-f1d7-48b1-aecf-4b2cec15b8bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 4: Write the resulting dataframe as a Delta table\n",
    "\n",
    "- For the first run use the `write` method\n",
    "- Later on use the `merge` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3c03668-2e65-4864-953f-ffde6530863d",
     "showTitle": true,
     "title": "Write"
    }
   },
   "outputs": [],
   "source": [
    "target_container = 'curated'\n",
    "target_path = f'wasbs://{target_container}@{storage_account_name}.blob.core.windows.net/movie_lens'\n",
    "\n",
    "# You can comment it after the first run\n",
    "resultDF.write.format(\"delta\").mode(\"overwrite\").partitionBy('rating_datetime').saveAsTable(path=target_path, name='movie_lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab8a528d-6344-4a1f-9145-15a03f02442a",
     "showTitle": true,
     "title": "Merge"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"app\").getOrCreate()\n",
    "target_table = DeltaTable.forPath(spark, target_path)\n",
    "\n",
    "key_column = \"movieId\"\n",
    "\n",
    "# merge\n",
    "(target_table.alias(\"target\")\n",
    "    .merge(resultDF.alias(\"source\"), f\"source.{key_column} = target.{key_column}\")\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92108b48-e8f5-4d0d-9d24-3775b36ca207",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Step 5: Query the data\n",
    "\n",
    "Now that we have created our Delta table, we can query it. For instance, you can identify particular columns to select and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a45cc5a-1fbd-40e9-8d29-50fad02a1b04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>movieId</th><th>count(1)</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "movieId",
            "nullable": true,
            "type": "integer"
           },
           {
            "metadata": {
             "__autoGeneratedAlias": "true"
            },
            "name": "count(1)",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 62
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "movieId",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from movie_lens"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3909535240367651,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Delta_Sample_Chapter5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
